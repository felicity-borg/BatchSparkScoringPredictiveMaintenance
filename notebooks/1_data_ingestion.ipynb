{"cells":[{"cell_type":"markdown","source":["# Step 1: Data ingestion\n\nThis notebook executes the first step of the data science process.\nIt downloads the raw data sets and stores them as Spark Dataframes accessible to your Azure Databricks instance. \nTo run all the cells included in the notebook click `Run All`— shown just above this cell at the top of the notebook—to generate the datasets required to execute the remaining notebooks for this use case. \n\n## Data sources\n\nThe common data elements for predictive maintenance problems can be summarized as follows:\n\n* Machine features: The features specific to each individual machine, e.g. engine size, make, model, location, installation date.\n* Telemetry data: The operating condition data collected from sensors, e.g. temperature, vibration, operating speeds, pressures.\n* Maintenance history: The repair history of a machine, e.g. maintenance activities or component replacements, this can also include error code or runtime message logs.\n* Failure history: The failure history of a machine or component of interest.\n\nIt is possible that failure history is contained within maintenance history, either as in the form of special error codes or order dates for spare parts. In those cases, failures can be extracted from the maintenance data. Additionally, different business domains may have a variety of other data sources that influence failure patterns which are not listed here exhaustively. These should be identified by consulting the domain experts when building predictive models.\n\nSome examples of above data elements from use cases are:\n    \n**Machine conditions and usage:** Flight routes and times, sensor data collected from aircraft engines, sensor readings from ATM transactions, train events data, sensor readings from wind turbines, elevators and connected cars.\n    \n**Machine features:** Circuit breaker technical specifications such as voltage levels, geolocation or car features such as make, model, engine size, tire types, production facility etc.\n\n**Failure history:** fight delay dates, aircraft component failure dates and types, ATM cash withdrawal transaction failures, train/elevator door failures, brake disk replacement order dates, wind turbine failure dates and circuit breaker command failures.\n\n**Maintenance history:** Flight error logs, ATM transaction error logs, train maintenance records including maintenance type, short description etc. and circuit breaker maintenance records.\n\nGiven the above data sources, the two main data types we observe in predictive maintenance domain are temporal data and static data. Failure history, machine conditions, repair history, usage history are time series indicated by the timestamp of data collection. Machine and operator specific features, are more static, since they usually describe the technical specifications of machines or operator’s properties.\n\nFor this scenario, we use a relatively large-scale data to walk you through the main steps from data ingestion (this Jupyter notebook), feature engineering, model building, and model operationalization and deployment. The code for the entire process is written in PySpark and implemented using Jupyter notebooks within Azure Databricks. \n\n# Step 1: Data Ingestion\n\nThis data aquisiton notebook will download the simulated predictive maintenance data sets from a GitHub data store. We do some preliminary data cleaning and store the results as a Spark data frame on the Azure Cluster for use in the remaining notebook steps of this use case.\n\n**Note:** This notebook will take about 8-10 minutes to execute all cells, depending on the compute configuration you have setup. Most of this time is spent handling the _telemetry_ data set, which contains about 8.7 million records."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6df65200-95fa-4fd3-a056-92db32c58c9e"}}},{"cell_type":"code","source":["## Setup our environment by importing required libraries\n\n# Github has been having some timeout issues. This should fix the problem for this dataset.\nimport socket\nsocket.setdefaulttimeout(90)\n\nimport glob\nimport os\n\n# Read csv file from URL directly\nimport pandas as pd\nimport urllib\nfrom datetime import datetime\n\n# Setup the pyspark environment\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15338ea0-149b-47ab-9306-ccfe0e63b38c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Download simulated data sets\n\nWe will be reusing the raw simulated data files from another tutorial. The notebook automatically downloads these files stored at [Microsoft/SQL-Server-R-Services-Samples GitHub site](https://github.com/Microsoft/SQL-Server-R-Services-Samples/tree/master/PredictiveMaintanenceModelingGuide/Data).\n\nThe five data files used in this use case are:\n\n * machines.csv\n * maint.csv\n * errors.csv\n * telemetry.csv\n * failures.csv\n \n **Input Data Overview** \n \n * `Telemetry.csv`: The telemetry time-series data consists of voltage, rotation, pressure and vibration measurements.\n * `Errors.csv`: The error logs contain non-breaking errors thrown while the machine is still operational and do not qualify as failures. <br> The error date and times are rounded to the closest hour since the telemetry data is collected at an hourly rate.\n * `Maint.csv`: The scheduled and unscheduled maintenance records which correspond to both regular inspection of components as well as failures. <br> A record is generated if a component is replaced during the scheduled inspection or replaced due to a break down.\n * `Machines.csv`: This data set includes machine model and age in years in service.\n * `Failures.csv`: These are the records of component replacements due to failures. Each record has a date and time, machine ID and failed component type associated with it.\n\nThere are 1000 machines of four different models. Each machine contains four components of interest, and four sensors measuring voltage, pressure, vibration and rotation. A controller monitors the system and raises alerts for five different error conditions. Maintenance logs indicate when something is done to the machine which does not include a component replacement. A failure is defined by the replacement of a component. \n\n![Machine Schemaic](https://raw.githubusercontent.com/Azure/BatchSparkScoringPredictiveMaintenance/master/images/machine.png)\n \nThis notebook does some preliminary data cleanup, creates summary graphics for each data set to verify the data downloaded correctly, and stores the resulting data sets in DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b01e4958-c7ab-4933-95c8-db57b9b23fc3"}}},{"cell_type":"code","source":["# The raw data is stored on GitHub here:\n# https://github.com/Microsoft/SQL-Server-R-Services-Samples/tree/master/PredictiveMaintenanceModelingGuide/Data\n# We access it through this URL:\nbasedataurl = \"https://media.githubusercontent.com/media/Microsoft/SQL-Server-R-Services-Samples/master/PredictiveMaintenanceModelingGuide/Data/\"\n\n# We will store each of these data sets in DBFS.\n\n# These file names detail which blob each files is stored under. \nMACH_DATA = 'machines_data'\nMAINT_DATA = 'maint_data'\nERROR_DATA = 'errors_data'\nTELEMETRY_DATA = 'telemetry_data'\nFAILURE_DATA = 'failure_data'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e026606d-8968-433d-90fd-ac28a659b9b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Machines data set\n\nThis simulation tracks a simulated set of 1000 machines over the course of a single year (2015). \n\nThis data set includes information about each machine: Machine ID, model type and age (years in service)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"63933c00-2c8a-4e35-84bc-928819d7e5ac"}}},{"cell_type":"code","source":["# load raw data from the GitHub URL\ndatafile = \"machines.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\nmachines = pd.read_csv(datafile)\n\n# The data was read in using a Pandas data frame. We'll convert \n# it to pyspark to ensure it is in a Spark usable form for later \n# manipulations.\nmach_spark = spark.createDataFrame(machines, \n                                   verifySchema=False)\n\n# Write the Machine data set to intermediate storage\nmach_spark.write.mode('overwrite').saveAsTable(MACH_DATA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92cd25af-f007-482b-9f8b-c892564f54b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"mach_spark","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"model","nullable":true,"type":"string"},{"metadata":{},"name":"age","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Errors  data set\n\nThe error log contains non-breaking errors recorded while the machine is still operational. These errors are not considered failures, though they may be predictive of a future failure event. The error datetime field is rounded to the closest hour since the telemetry data (loaded later) is collected on an hourly rate."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7222f17-e7cf-4b7c-84f4-1b6f96fa68b8"}}},{"cell_type":"code","source":["# load raw data from the GitHub URL\ndatafile = \"errors.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\nerrors = pd.read_csv(datafile, encoding='utf-8')\n\n# The data was read in using a Pandas data frame. We'll convert \n# it to pyspark to ensure it is in a Spark usable form for later \n# manipulations.\nerror_spark = spark.createDataFrame(errors, \n                               verifySchema=False)\n\n# Write the Errors data set to intermediate storage\nerror_spark.write.mode('overwrite').saveAsTable(ERROR_DATA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9fa20bf-a23c-423a-b498-1e219aa36518"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"error_spark","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"datetime","nullable":true,"type":"string"},{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"errorID","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Maintenance data set\n\nThe maintenance log contains both scheduled and unscheduled maintenance records. Scheduled maintenance corresponds with  regular inspection of components, unscheduled maintenance may arise from mechanical failure or other performance degradations. A failure record is generated for component replacement in the case  of either maintenance events. Because maintenance events can also be used to infer component life, the maintenance data has been collected over two years (2014, 2015) instead of only over the year of interest (2015)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e3b2789-2dae-4fd3-8487-65f2c517555c"}}},{"cell_type":"code","source":["# load raw data from the GitHub URL\ndatafile = \"maint.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\nmaint = pd.read_csv(datafile, encoding='utf-8')\n\n# The data was read in using a Pandas data frame. We'll convert \n# it to pyspark to ensure it is in a Spark usable form for later \n# manipulations.\nmaint_spark = spark.createDataFrame(maint, \n                              verifySchema=False)\n\n# Write the Maintenance data set to intermediate storage\nmaint_spark.write.mode('overwrite').saveAsTable(MAINT_DATA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2a13b5d-04a3-4c3a-a66a-221f71fe4607"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"maint_spark","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"datetime","nullable":true,"type":"string"},{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"comp","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Telemetry data set\n\nThe telemetry time-series data consists of voltage, rotation, pressure, and vibration sensor measurements collected from each  machines in real time. The data is averaged over an hour and stored in the telemetry logs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d97faa5b-d2c3-45f0-ac65-046a003000e6"}}},{"cell_type":"code","source":["# load raw data from the GitHub URL\ndatafile = \"telemetry.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\ntelemetry = pd.read_csv(datafile, encoding='utf-8')\n\n# handle missing values\n# define groups of features \nfeatures_datetime = ['datetime']\nfeatures_categorical = ['machineID']\nfeatures_numeric = list(set(telemetry.columns) - set(features_datetime) - set(features_categorical))\n\n# Replace numeric NA with 0\ntelemetry[features_numeric] = telemetry[features_numeric].fillna(0)\n\n# Replace categorical NA with 'Unknown'\ntelemetry[features_categorical]  = telemetry[features_categorical].fillna(\"Unknown\")\n\n# The data was read in using a Pandas data frame. We'll convert \n# it to pyspark to ensure it is in a Spark usable form for later \n# manipulations.\n# This line takes about 9.5 minutes to run.\ntelemetry_spark = spark.createDataFrame(telemetry, verifySchema=False)\n\n# Write the telemetry data set to intermediate storage\ntelemetry_spark.write.mode('overwrite').saveAsTable(TELEMETRY_DATA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fae7a7c-7d7e-4223-bc86-010991d08893"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"telemetry_spark","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"datetime","nullable":true,"type":"string"},{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"volt","nullable":true,"type":"double"},{"metadata":{},"name":"rotate","nullable":true,"type":"double"},{"metadata":{},"name":"pressure","nullable":true,"type":"double"},{"metadata":{},"name":"vibration","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Failures data set\n\nFailures correspond to component replacements within the maintenance log. Each record contains the Machine ID, component type, and replacement datetime. These records will be used to create the machine learning labels we will be trying to predict."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf369cd5-a7eb-410a-909b-5ac854fa3eb6"}}},{"cell_type":"code","source":["# load raw data from the GitHub URL\ndatafile = \"failures.csv\"\n\n# Download the file once, and only once.\nif not os.path.isfile(datafile):\n    urllib.request.urlretrieve(basedataurl+datafile, datafile)\n    \n# Read into pandas\nfailures = pd.read_csv(datafile, encoding='utf-8')\n\n# The data was read in using a Pandas data frame. We'll convert \n# it to pyspark to ensure it is in a Spark usable form for later \n# manipulations.\nfailures_spark = spark.createDataFrame(failures, \n                                       verifySchema=False)\n\n# Write the failures data set to intermediate storage\nfailures_spark.write.mode('overwrite').saveAsTable(FAILURE_DATA)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ef84fc1-8f38-4efb-9e2e-4c5955e081aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"failures_spark","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"datetime","nullable":true,"type":"string"},{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"failure","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Conclusion\n\n\nWe have now stored the Raw data required for this Predictive Maintenance scenario as Spark data frames in the Azure Databricks instance. You can examine them in the Data panel accessible on the left. You should see the following five data sources:\n\n 1. error_files\n 1. machine_files\n 1. maint_files\n 1. telemetry_files\n 1. failure_files"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92065855-4767-47be-af86-406ebc498f1f"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0173c6d0-1082-4286-8e90-303f90b8b53c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1_Data_Ingestion","dashboards":[],"language":"python","widgets":{},"notebookOrigID":163241296183503}},"nbformat":4,"nbformat_minor":0}
