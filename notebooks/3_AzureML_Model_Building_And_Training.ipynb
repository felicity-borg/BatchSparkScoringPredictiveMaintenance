{"cells":[{"cell_type":"markdown","source":["# Model Building and Training with Databricks and Azure ML Services"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a3aa165-4c77-4448-844b-7eac1b915a11"}}},{"cell_type":"markdown","source":["This notebook constructs a machine learning model designed to predict component failure in the machine. <br> This includes running the `2_Feature_engineering` notebooks which takes the raw data as it would arrive from the machines we're interested in, <br> manipulates and transforms the raw data sets into a training data set which we then use to train the machine learning model to accurately predict the outcome of interest. <br> You must have already run the 1_data_ingestion notebook to download the raw predictive maintenance scenario data before running this notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e0de7d1-fa80-4743-91ab-8865feae7142"}}},{"cell_type":"markdown","source":["## Setup\n\nThe `2_Feature_Engineering` notebook takes parameters for which model to build (model), where to store the training data (features_table), and the start (start_date) and end (to_date) dates to use when creating the training data. \n\nUsing these parameters, it creates the training data by calling the `./notebooks/2_Feature_Engineering` with the correct parameters. When the `./notebooks/2_Feature_Engineering` notebook completes, we can start running the other cells to build our model and start tracking our Experiment on Azure Machine Learning Services."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fefec5d4-b675-4fb9-a36a-6496d666bfe6"}}},{"cell_type":"code","source":["# Setup our environment by importing required libraries and secifying what data we want to examine using databricks parameters.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n#Used data files\ntraining_table= 'training_data'\nmodel_type = 'RandomForest' \n\n#Databricks paramaters to customize the runs\n# Input widgets allow you to add parameters to your notebooks and dashboards\ndbutils.widgets.removeAll()\ndbutils.widgets.text(\"features_table\", training_table)\ndbutils.widgets.text(\"model\", model_type)\n\n\ndbutils.widgets.text(\"start_date\", '2000-01-01')\ndbutils.widgets.text(\"to_date\", '2015-10-30')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"635e1b89-77e3-4cff-ad29-587d04b6a48b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":["Model","Testing_table"],"addedWidgets":{"features_table":{"name":"features_table","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"training_data"},"model":{"name":"model","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"RandomForest"},"start_date":{"name":"start_date","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"2000-01-01"},"to_date":{"name":"to_date","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"2015-10-30"}},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Feature Engineering\n\nThe `2_Feature_Engineering` notebook run below creates a labeled training data set using the parameters `start_date` and `to_date` to select the time period for training. This data set is stored in the `features_table` specified. After this cell completes, you should see the dataset named `training_data` under the Databricks `Data` icon."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99667fc8-bf5c-4a66-b237-a7992223c419"}}},{"cell_type":"code","source":["# Run the feature engineering notebook\n\ndbutils.notebook.run(\"2_Feature_Engineering\", 600, {\"features_table\": dbutils.widgets.get(\"features_table\"), \n                                                     \"start_date\": dbutils.widgets.get(\"start_date\"), \n                                                     \"to_date\": dbutils.widgets.get(\"to_date\")})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78696193-c38a-4cbf-b1bf-4c9135fa1d7d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{"features_table":"training_data","start_date":"2000-01-01","to_date":"2015-10-30"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Load the Azure ML Workspace\n\nBefore the model can be deployed and tracked on Azure ML, you must first create or define your Azure ML Workspace object. \n\nThis defines the workspace you will be deploying your model to. \n\nCreating the object can be done by passing the name of your workspace, your Azure subscription ID and the resource group where your Azure ML workspace is located.\n\nFor more information about creating an Azure ML workspace, see the [Azure ML Workspace managmenet documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=python)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be79dfa1-7ebf-4cbf-850a-bef34d392d68"}}},{"cell_type":"code","source":["##Load an Azure Ml Workspace to link Databricks workspace to Azure Machine Learning Service\n\nimport mlflow\nimport mlflow.azureml\nfrom azureml.core import Workspace\n\nsubscription_id = '52cbf6c7-01f2-4df2-bae9-c80cee4db7eb'\n\n# Azure Machine Learning resource group NOT the managed resource group\nresource_group = 'Peak-MLTemplates-RG' \n\n#Azure Machine Learning workspace name, NOT Azure Databricks workspace\nworkspace_name = 'peak-PM-ws'  \n\n# Instantiate Azure Machine Learning workspace\nws = Workspace.get(name=workspace_name,\n                   subscription_id=subscription_id,\n                   resource_group=resource_group)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"562ec393-d248-4065-83c9-dd69cb43d142"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Performing interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CRKZAMNB6 to authenticate.\nYou have logged in. Now let us find all the subscriptions to which you have access...\nInteractive authentication successfully completed.\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Performing interactive authentication. Please follow the instructions on the terminal.\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code CRKZAMNB6 to authenticate.\nYou have logged in. Now let us find all the subscriptions to which you have access...\nInteractive authentication successfully completed.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Load the training data and add databricks paramaters"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"330259af-7082-4ced-b540-8b195fb39e36"}}},{"cell_type":"code","source":["#Databricks paramaters to customize the runs\n\nspark = SparkSession.builder.getOrCreate()\ndbutils.widgets.text(\"training_table\",training_table)\ndbutils.widgets.text(\"Model\", model_type)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89a56399-807f-417c-95bc-92e05df5bffe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{"training_table":{"name":"training_table","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"training_data"},"Model":{"name":"Model","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"RandomForest"}},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.catalog.refreshTable(dbutils.widgets.get(\"training_table\")) \ntrain_data = spark.table(dbutils.widgets.get(\"training_table\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7daafca7-c78d-4110-bf6d-6bccb2a5ef61"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"train_data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"dt_truncated","nullable":true,"type":"timestamp"},{"metadata":{},"name":"volt_rollingmean_12","nullable":true,"type":"double"},{"metadata":{},"name":"rotate_rollingmean_12","nullable":true,"type":"double"},{"metadata":{},"name":"pressure_rollingmean_12","nullable":true,"type":"double"},{"metadata":{},"name":"vibration_rollingmean_12","nullable":true,"type":"double"},{"metadata":{},"name":"volt_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"rotate_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"pressure_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"vibration_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"volt_rollingmean_36","nullable":true,"type":"double"},{"metadata":{},"name":"vibration_rollingmean_36","nullable":true,"type":"double"},{"metadata":{},"name":"rotate_rollingmean_36","nullable":true,"type":"double"},{"metadata":{},"name":"pressure_rollingmean_36","nullable":true,"type":"double"},{"metadata":{},"name":"volt_rollingstd_12","nullable":true,"type":"double"},{"metadata":{},"name":"rotate_rollingstd_12","nullable":true,"type":"double"},{"metadata":{},"name":"pressure_rollingstd_12","nullable":true,"type":"double"},{"metadata":{},"name":"vibration_rollingstd_12","nullable":true,"type":"double"},{"metadata":{},"name":"volt_rollingstd_24","nullable":true,"type":"double"},{"metadata":{},"name":"rotate_rollingstd_24","nullable":true,"type":"double"},{"metadata":{},"name":"pressure_rollingstd_24","nullable":true,"type":"double"},{"metadata":{},"name":"vibration_rollingstd_24","nullable":true,"type":"double"},{"metadata":{},"name":"volt_rollingstd_36","nullable":true,"type":"double"},{"metadata":{},"name":"rotate_rollingstd_36","nullable":true,"type":"double"},{"metadata":{},"name":"pressure_rollingstd_36","nullable":true,"type":"double"},{"metadata":{},"name":"vibration_rollingstd_36","nullable":true,"type":"double"},{"metadata":{},"name":"error1sum_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"error2sum_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"error3sum_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"error4sum_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"error5sum_rollingmean_24","nullable":true,"type":"double"},{"metadata":{},"name":"comp1sum","nullable":true,"type":"double"},{"metadata":{},"name":"comp2sum","nullable":true,"type":"double"},{"metadata":{},"name":"comp3sum","nullable":true,"type":"double"},{"metadata":{},"name":"comp4sum","nullable":true,"type":"double"},{"metadata":{},"name":"model","nullable":true,"type":"string"},{"metadata":{},"name":"age","nullable":true,"type":"long"},{"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":0,"name":"model3"},{"idx":1,"name":"model4"},{"idx":2,"name":"model2"}]},"num_attrs":3}},"name":"model_encoded","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{},"name":"failure","nullable":true,"type":"double"},{"metadata":{},"name":"label_e","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{"training_table":"training_data"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Prepare the Training data\n\nA fundamental practice in machine learning is to calibrate and test your model parameters on data that has not been used to train the model. <br> Evaluation of the model requires splitting the available data into a training portion, a calibration portion and an evaluation portion.<br> Typically, 80% of data is used to train the model and 10% each to calibrate any parameter selection and evaluate your model.\n\nIn general random splitting can be used, but since time series data have an inherent correlation between observations; for predictive maintenance problems,<br> a time-dependent spliting strategy is often a better approach to estimate performance. <br> For a time-dependent split, a single point in time is chosen, the model is trained on examples up to that point in time, and validated on the examples after that point. <br> This simulates training on current data and score data collected in the future data after the splitting point is not known. <br> However, care must be taken on labels near the split point. <br> In this case, feature records within 7 days of the split point can not be labeled as a failure, since that is unobserved data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f39c55e6-6d47-4a70-9fa5-caf92f34feda"}}},{"cell_type":"code","source":["# define list of input columns for downstream modeling\n\n# We'll use the known label, and key variables.\nlabel_var = ['label_e']\nkey_cols =['machineID','dt_truncated']\n\n# Then get the remaining feature names from the data\ninput_features = train_data.columns\n\n# Remove the known label, key variables and a few extra columns we won't need.\nremove_names = label_var + key_cols + ['failure','model_encoded','model' ]\n\n# Create the iout features \ninput_features = [x for x in input_features if x not in set(remove_names)]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2cc40cb-85c4-4802-a412-c4e6e680c51a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Spark models require a vectorized data frame. We transform the dataset here and then split the data into a training and test set. <br>\nWe use this split data to train the model on 9 months of data (training data), and evaluate on the remaining 3 months (test data) going forward."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb75bdff-1e27-4267-8242-d102697b2691"}}},{"cell_type":"code","source":["# Import the libraries \nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import DecisionTreeClassifier\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# for creating pipelines and model\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n\n# assemble features\nva = VectorAssembler(inputCols=(input_features), outputCol='features')\ntrain_data = va.transform(train_data).select('machineID','dt_truncated','label_e','features')\n\n# set maxCategories so features with > 10 distinct values are treated as continuous.\nfeatureIndexer = VectorIndexer(inputCol=\"features\", \n                               outputCol=\"indexedFeatures\", \n                               maxCategories=10).fit(train_data)\n\n# fit on whole dataset to include all labels in index\nlabelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(train_data)\n\ntraining = train_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a5f9330-7e40-48ea-8e4a-a2e5c66f00b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"train_data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"dt_truncated","nullable":true,"type":"timestamp"},{"metadata":{},"name":"label_e","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"volt_rollingmean_12"},{"idx":1,"name":"rotate_rollingmean_12"},{"idx":2,"name":"pressure_rollingmean_12"},{"idx":3,"name":"vibration_rollingmean_12"},{"idx":4,"name":"volt_rollingmean_24"},{"idx":5,"name":"rotate_rollingmean_24"},{"idx":6,"name":"pressure_rollingmean_24"},{"idx":7,"name":"vibration_rollingmean_24"},{"idx":8,"name":"volt_rollingmean_36"},{"idx":9,"name":"vibration_rollingmean_36"},{"idx":10,"name":"rotate_rollingmean_36"},{"idx":11,"name":"pressure_rollingmean_36"},{"idx":12,"name":"volt_rollingstd_12"},{"idx":13,"name":"rotate_rollingstd_12"},{"idx":14,"name":"pressure_rollingstd_12"},{"idx":15,"name":"vibration_rollingstd_12"},{"idx":16,"name":"volt_rollingstd_24"},{"idx":17,"name":"rotate_rollingstd_24"},{"idx":18,"name":"pressure_rollingstd_24"},{"idx":19,"name":"vibration_rollingstd_24"},{"idx":20,"name":"volt_rollingstd_36"},{"idx":21,"name":"rotate_rollingstd_36"},{"idx":22,"name":"pressure_rollingstd_36"},{"idx":23,"name":"vibration_rollingstd_36"},{"idx":24,"name":"error1sum_rollingmean_24"},{"idx":25,"name":"error2sum_rollingmean_24"},{"idx":26,"name":"error3sum_rollingmean_24"},{"idx":27,"name":"error4sum_rollingmean_24"},{"idx":28,"name":"error5sum_rollingmean_24"},{"idx":29,"name":"comp1sum"},{"idx":30,"name":"comp2sum"},{"idx":31,"name":"comp3sum"},{"idx":32,"name":"comp4sum"},{"idx":33,"name":"age"}]},"num_attrs":34}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"training","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"dt_truncated","nullable":true,"type":"timestamp"},{"metadata":{},"name":"label_e","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"volt_rollingmean_12"},{"idx":1,"name":"rotate_rollingmean_12"},{"idx":2,"name":"pressure_rollingmean_12"},{"idx":3,"name":"vibration_rollingmean_12"},{"idx":4,"name":"volt_rollingmean_24"},{"idx":5,"name":"rotate_rollingmean_24"},{"idx":6,"name":"pressure_rollingmean_24"},{"idx":7,"name":"vibration_rollingmean_24"},{"idx":8,"name":"volt_rollingmean_36"},{"idx":9,"name":"vibration_rollingmean_36"},{"idx":10,"name":"rotate_rollingmean_36"},{"idx":11,"name":"pressure_rollingmean_36"},{"idx":12,"name":"volt_rollingstd_12"},{"idx":13,"name":"rotate_rollingstd_12"},{"idx":14,"name":"pressure_rollingstd_12"},{"idx":15,"name":"vibration_rollingstd_12"},{"idx":16,"name":"volt_rollingstd_24"},{"idx":17,"name":"rotate_rollingstd_24"},{"idx":18,"name":"pressure_rollingstd_24"},{"idx":19,"name":"vibration_rollingstd_24"},{"idx":20,"name":"volt_rollingstd_36"},{"idx":21,"name":"rotate_rollingstd_36"},{"idx":22,"name":"pressure_rollingstd_36"},{"idx":23,"name":"vibration_rollingstd_36"},{"idx":24,"name":"error1sum_rollingmean_24"},{"idx":25,"name":"error2sum_rollingmean_24"},{"idx":26,"name":"error3sum_rollingmean_24"},{"idx":27,"name":"error4sum_rollingmean_24"},{"idx":28,"name":"error5sum_rollingmean_24"},{"idx":29,"name":"comp1sum"},{"idx":30,"name":"comp2sum"},{"idx":31,"name":"comp3sum"},{"idx":32,"name":"comp4sum"},{"idx":33,"name":"age"}]},"num_attrs":34}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Prepare the Testing Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"634f7a2d-1d84-4048-b6e5-1e0fa856676d"}}},{"cell_type":"markdown","source":["To evaluate this model, we predict the component failures over the test data set.<br> Since the test set has been created from data the model has not been seen before, it simulates future data. <br> The evaluation can then be generalized to assess how the model could perform when operationalized and used to score new data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3be2834b-db98-4594-b183-9932d41943ce"}}},{"cell_type":"code","source":["# Add databricks paramaters\n\ntesting_table = 'testing_data'\ndbutils.widgets.removeAll()\ndbutils.widgets.text(\"Testing_table\",testing_table)\ndbutils.widgets.text(\"Model\", model_type)\ndbutils.widgets.text(\"start_date\", '2015-11-30')\ndbutils.widgets.text(\"to_date\", '2016-02-01')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19378576-6c53-42a9-aca5-a1c1990c1990"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":["model","training_table","features_table"],"addedWidgets":{"Testing_table":{"name":"Testing_table","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"testing_data"},"start_date":{"name":"start_date","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"2015-11-30"},"Model":{"name":"Model","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"RandomForest"},"to_date":{"name":"to_date","label":null,"widgetType":"text","options":{"widgetType":"text","validationRegex":null},"defaultValue":"2016-02-01"}},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.catalog.setCurrentDatabase(\"default\")\nexists = False\nfor tbl in spark.catalog.listTables():\n  if tbl.name == dbutils.widgets.get(\"Testing_table\"):\n    exists = True\n    break"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"476fcf09-5ad0-458d-bc77-efe01731007e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{"Testing_table":"testing_data"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if not exists:\n  dbutils.notebook.run(\"2_Feature_Engineering\", 600, {\"features_table\": dbutils.widgets.get(\"Testing_table\"), \n                                                       \"start_date\": dbutils.widgets.get(\"start_date\"), \n                                                       \"to_date\": dbutils.widgets.get(\"to_date\")})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2800f39a-2d83-42e6-8bc4-f69106d02bad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#Load the data\n\ntest_data = spark.table(dbutils.widgets.get(\"Testing_table\"))\n\n# Testing data is prepared using the same steps used for the traning data \n\n# define list of input columns for downstream modeling\n\n# We'll use the known label, and key variables.\nlabel_var = ['label_e']\nkey_cols =['machineID','dt_truncated']\n\n# Then get the remaining feature names from the data\ninput_features = test_data.columns\n\n# Remove the known label, key variables and a few extra columns we won't need.\nremove_names = label_var + key_cols + ['failure','model_encoded','model' ]\n\n# Create the iout features \ninput_features = [x for x in input_features if x not in set(remove_names)]\n\n# assemble features\nva = VectorAssembler(inputCols=(input_features), outputCol='features')\n\n# assemble features\ntest_data = va.transform(test_data).select('machineID','dt_truncated','label_e','features')\n\n# set maxCategories so features with > 10 distinct values are treated as continuous.\nfeatureIndexer = VectorIndexer(inputCol=\"features\", \n                               outputCol=\"indexedFeatures\", \n                               maxCategories=10).fit(test_data)\n\n# fit on whole dataset to include all labels in index\nlabelIndexer = StringIndexer(inputCol=\"label_e\", outputCol=\"indexedLabel\").fit(test_data)\n\ntesting = test_data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3a6087ce-190e-4e6a-9079-697c6b132677"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"test_data","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"dt_truncated","nullable":true,"type":"timestamp"},{"metadata":{},"name":"label_e","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"volt_rollingmean_12"},{"idx":1,"name":"rotate_rollingmean_12"},{"idx":2,"name":"pressure_rollingmean_12"},{"idx":3,"name":"vibration_rollingmean_12"},{"idx":4,"name":"volt_rollingmean_24"},{"idx":5,"name":"rotate_rollingmean_24"},{"idx":6,"name":"pressure_rollingmean_24"},{"idx":7,"name":"vibration_rollingmean_24"},{"idx":8,"name":"volt_rollingmean_36"},{"idx":9,"name":"vibration_rollingmean_36"},{"idx":10,"name":"rotate_rollingmean_36"},{"idx":11,"name":"pressure_rollingmean_36"},{"idx":12,"name":"volt_rollingstd_12"},{"idx":13,"name":"rotate_rollingstd_12"},{"idx":14,"name":"pressure_rollingstd_12"},{"idx":15,"name":"vibration_rollingstd_12"},{"idx":16,"name":"volt_rollingstd_24"},{"idx":17,"name":"rotate_rollingstd_24"},{"idx":18,"name":"pressure_rollingstd_24"},{"idx":19,"name":"vibration_rollingstd_24"},{"idx":20,"name":"volt_rollingstd_36"},{"idx":21,"name":"rotate_rollingstd_36"},{"idx":22,"name":"pressure_rollingstd_36"},{"idx":23,"name":"vibration_rollingstd_36"},{"idx":24,"name":"error1sum_rollingmean_24"},{"idx":25,"name":"error2sum_rollingmean_24"},{"idx":26,"name":"error3sum_rollingmean_24"},{"idx":27,"name":"error4sum_rollingmean_24"},{"idx":28,"name":"error5sum_rollingmean_24"},{"idx":29,"name":"comp1sum"},{"idx":30,"name":"comp2sum"},{"idx":31,"name":"comp3sum"},{"idx":32,"name":"comp4sum"},{"idx":33,"name":"age"}]},"num_attrs":34}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null},{"name":"testing","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"dt_truncated","nullable":true,"type":"timestamp"},{"metadata":{},"name":"label_e","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"volt_rollingmean_12"},{"idx":1,"name":"rotate_rollingmean_12"},{"idx":2,"name":"pressure_rollingmean_12"},{"idx":3,"name":"vibration_rollingmean_12"},{"idx":4,"name":"volt_rollingmean_24"},{"idx":5,"name":"rotate_rollingmean_24"},{"idx":6,"name":"pressure_rollingmean_24"},{"idx":7,"name":"vibration_rollingmean_24"},{"idx":8,"name":"volt_rollingmean_36"},{"idx":9,"name":"vibration_rollingmean_36"},{"idx":10,"name":"rotate_rollingmean_36"},{"idx":11,"name":"pressure_rollingmean_36"},{"idx":12,"name":"volt_rollingstd_12"},{"idx":13,"name":"rotate_rollingstd_12"},{"idx":14,"name":"pressure_rollingstd_12"},{"idx":15,"name":"vibration_rollingstd_12"},{"idx":16,"name":"volt_rollingstd_24"},{"idx":17,"name":"rotate_rollingstd_24"},{"idx":18,"name":"pressure_rollingstd_24"},{"idx":19,"name":"vibration_rollingstd_24"},{"idx":20,"name":"volt_rollingstd_36"},{"idx":21,"name":"rotate_rollingstd_36"},{"idx":22,"name":"pressure_rollingstd_36"},{"idx":23,"name":"vibration_rollingstd_36"},{"idx":24,"name":"error1sum_rollingmean_24"},{"idx":25,"name":"error2sum_rollingmean_24"},{"idx":26,"name":"error3sum_rollingmean_24"},{"idx":27,"name":"error4sum_rollingmean_24"},{"idx":28,"name":"error5sum_rollingmean_24"},{"idx":29,"name":"comp1sum"},{"idx":30,"name":"comp2sum"},{"idx":31,"name":"comp3sum"},{"idx":32,"name":"comp4sum"},{"idx":33,"name":"age"}]},"num_attrs":34}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{"Testing_table":"testing_data"}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Classification Models\n\nA particular problem in predictive maintenance is machine failures are usually rare occurrences compared to normal operation. This is fortunate for the business as maintenance and saftey issues are few, but causes an imbalance in the label distribution. This imbalance leads to poor performance as algorithms tend to classify majority class examples at the expense of minority class, since the total misclassification error is much improved when majority class is labeled correctly. This causes low recall or precision rates, although accuracy can be high. It becomes a larger problem when the cost of false alarms is very high. To help with this problem, sampling techniques such as oversampling of the minority examples can be used. These methods are not covered in this notebook. Because of this, it is also important to look at evaluation metrics other than accuracy alone.\n\nWe will build a Random Forest Classifier:\n\n- **Random Forest Classifier**: A random forest is an ensemble of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. Tree ensemble algorithms such as random forests and boosting are among the top performers for classification and regression tasks.\n\nThe next code block creates the model. A series of model hyperparametershave also been included to guide your exploration of the model space."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7f8506b-ff94-433c-abb7-694b32f1934a"}}},{"cell_type":"code","source":["# import the libraries for creating pipelines and model\n\nimport mlflow\nimport mlflow.spark\nfrom pyspark.sql import SparkSession\nimport numpy as np\nimport mlflow.pyfunc\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, VectorIndexer\n\n# Start the experient which we can now start tracking on Azure ML Service and build the model\n\nwith mlflow.start_run(): # Naming it will allow you to register the model\n  rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", \n                              # Maximum depth of the tree. (>= 0) \n                              # E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'\n                              maxDepth=15,\n                              # Max number of bins for discretizing continuous features. \n                              # Must be >=2 and >= number of categories for any categorical feature.\n                              maxBins=32,\n                              # Minimum number of instances each child must have after split. \n                              # If a split causes the left or right child to have fewer than \n                              # minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.\n                              minInstancesPerNode=1,\n                              # Minimum information gain for a split to be considered at a tree node.\n                              minInfoGain=0.0,\n                              # Criterion used for information gain calculation (case-insensitive). \n                              # Supported options: entropy, gini')\n                              impurity=\"gini\",\n                              # Number of trees to train (>= 1)\n                              numTrees=200, \n                              # The number of features to consider for splits at each tree node. \n                              # Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].\n                              featureSubsetStrategy=\"sqrt\", \n                              # Fraction of the training data used for learning each  \n                              # decision tree, in range (0, 1].' \n                              subsamplingRate = 0.632)\n  \n  # chain indexers and model in a Pipeline\n  pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])\n  \n  # train model.  This also runs the indexers.\n  model = pipeline.fit(training)\n  \n  \n  # Evaluate the model \n  \n  # make predictions. The Pipeline does all the same operations on the test data\n  predictions = model.transform(testing)\n  \n  # Create the confusion matrix for the multiclass prediction results\n  # This result assumes a decision boundary of p = 0.5\n  conf_table = predictions.stat.crosstab('indexedLabel', 'prediction')\n  confuse = conf_table.toPandas()\n  confuse.head()\n  \n  # Log MLflow Metrics and Model\n  # select (prediction, true label) and compute test error\n  # select (prediction, true label) and compute test error\n  # True positives - diagonal failure terms\n  tp = confuse['1.0'][1]+confuse['2.0'][2]+confuse['3.0'][3]+confuse['4.0'][4]\n  # False positves - All failure terms - True positives\n  fp = np.sum(np.sum(confuse[['1.0', '2.0','3.0','4.0']])) - tp\n  # True negatives \n  tn = confuse['0.0'][0]\n  # False negatives total of non-failure column - TN\n  fn = np.sum(np.sum(confuse[['0.0']])) - tn\n  \n  # Accuracy is diagonal/total \n  acc_n = tn + tp\n  acc_d = np.sum(np.sum(confuse[['0.0','1.0', '2.0','3.0','4.0']]))\n  acc = acc_n/acc_d\n  \n  # Calculate precision and recall.\n  prec = tp/(tp+fp)\n  rec = tp/(tp+fn)\n  \n  # Calculate F1\n  FOne = 2.0 * prec * rec/(prec + rec)\n  \n  # These metrics are logged onto Azure ML Service where you can track them\n  \n  # Log the evaluation metrics and model\n  mlflow.log_metric(\"acc\", acc)\n  mlflow.log_metric(\"pre\", prec)\n  mlflow.log_metric(\"rec\", rec)\n  mlflow.log_metric(\"f1\", FOne)\n  \n  # Log and register the model\n  mlflow.spark.log_model(model, artifact_path = \"model\",\n                        registered_model_name = \"PM-RandomForest\")\n  \n  last_run_id = mlflow.active_run().info.run_id\n\n  # Print the evaluation metrics to the notebook\n  print(\"Accuracy = %g\" % acc)\n  print(\"Precision = %g\" % prec)\n  print(\"Recall = %g\" % rec )\n  print(\"F1 = %g\" % (2.0 * prec * rec/(prec + rec)))\n  print(\"\")\n  \n  mlflow.end_run()\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"91bc622e-4886-4553-9470-a67d0d794779"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"predictions","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"machineID","nullable":true,"type":"long"},{"metadata":{},"name":"dt_truncated","nullable":true,"type":"timestamp"},{"metadata":{},"name":"label_e","nullable":true,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"volt_rollingmean_12"},{"idx":1,"name":"rotate_rollingmean_12"},{"idx":2,"name":"pressure_rollingmean_12"},{"idx":3,"name":"vibration_rollingmean_12"},{"idx":4,"name":"volt_rollingmean_24"},{"idx":5,"name":"rotate_rollingmean_24"},{"idx":6,"name":"pressure_rollingmean_24"},{"idx":7,"name":"vibration_rollingmean_24"},{"idx":8,"name":"volt_rollingmean_36"},{"idx":9,"name":"vibration_rollingmean_36"},{"idx":10,"name":"rotate_rollingmean_36"},{"idx":11,"name":"pressure_rollingmean_36"},{"idx":12,"name":"volt_rollingstd_12"},{"idx":13,"name":"rotate_rollingstd_12"},{"idx":14,"name":"pressure_rollingstd_12"},{"idx":15,"name":"vibration_rollingstd_12"},{"idx":16,"name":"volt_rollingstd_24"},{"idx":17,"name":"rotate_rollingstd_24"},{"idx":18,"name":"pressure_rollingstd_24"},{"idx":19,"name":"vibration_rollingstd_24"},{"idx":20,"name":"volt_rollingstd_36"},{"idx":21,"name":"rotate_rollingstd_36"},{"idx":22,"name":"pressure_rollingstd_36"},{"idx":23,"name":"vibration_rollingstd_36"},{"idx":24,"name":"error1sum_rollingmean_24"},{"idx":25,"name":"error2sum_rollingmean_24"},{"idx":26,"name":"error3sum_rollingmean_24"},{"idx":27,"name":"error4sum_rollingmean_24"},{"idx":28,"name":"error5sum_rollingmean_24"},{"idx":29,"name":"comp1sum"},{"idx":30,"name":"comp2sum"},{"idx":31,"name":"comp3sum"},{"idx":32,"name":"comp4sum"},{"idx":33,"name":"age"}]},"num_attrs":34}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"ml_attr":{"name":"indexedLabel","type":"nominal","vals":["0.0","2.0","1.0","4.0","3.0"]}},"name":"indexedLabel","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"numeric":[{"idx":0,"name":"volt_rollingmean_12"},{"idx":1,"name":"rotate_rollingmean_12"},{"idx":2,"name":"pressure_rollingmean_12"},{"idx":3,"name":"vibration_rollingmean_12"},{"idx":4,"name":"volt_rollingmean_24"},{"idx":5,"name":"rotate_rollingmean_24"},{"idx":6,"name":"pressure_rollingmean_24"},{"idx":7,"name":"vibration_rollingmean_24"},{"idx":8,"name":"volt_rollingmean_36"},{"idx":9,"name":"vibration_rollingmean_36"},{"idx":10,"name":"rotate_rollingmean_36"},{"idx":11,"name":"pressure_rollingmean_36"},{"idx":12,"name":"volt_rollingstd_12"},{"idx":13,"name":"rotate_rollingstd_12"},{"idx":14,"name":"pressure_rollingstd_12"},{"idx":15,"name":"vibration_rollingstd_12"},{"idx":16,"name":"volt_rollingstd_24"},{"idx":17,"name":"rotate_rollingstd_24"},{"idx":18,"name":"pressure_rollingstd_24"},{"idx":19,"name":"vibration_rollingstd_24"},{"idx":20,"name":"volt_rollingstd_36"},{"idx":21,"name":"rotate_rollingstd_36"},{"idx":22,"name":"pressure_rollingstd_36"},{"idx":23,"name":"vibration_rollingstd_36"},{"idx":24,"name":"error1sum_rollingmean_24"},{"idx":25,"name":"error2sum_rollingmean_24"},{"idx":26,"name":"error3sum_rollingmean_24"},{"idx":27,"name":"error4sum_rollingmean_24"},{"idx":28,"name":"error5sum_rollingmean_24"},{"idx":29,"name":"comp1sum"},{"idx":30,"name":"comp2sum"},{"idx":31,"name":"comp3sum"},{"idx":32,"name":"comp4sum"},{"idx":33,"name":"age"}]},"num_attrs":34}},"name":"indexedFeatures","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"ml_attr":{"num_attrs":5}},"name":"rawPrediction","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"ml_attr":{"num_attrs":5}},"name":"probability","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"ml_attr":{"num_vals":5,"type":"nominal"}},"name":"prediction","nullable":false,"type":"double"}],"type":"struct"},"tableIdentifier":null},{"name":"conf_table","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"indexedLabel_prediction","nullable":true,"type":"string"},{"metadata":{},"name":"0.0","nullable":true,"type":"long"},{"metadata":{},"name":"1.0","nullable":true,"type":"long"},{"metadata":{},"name":"2.0","nullable":true,"type":"long"},{"metadata":{},"name":"3.0","nullable":true,"type":"long"},{"metadata":{},"name":"4.0","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">Registered model &#39;PM-RandomForest&#39; already exists. Creating a new version of this model...\nCreated version &#39;2&#39; of model &#39;PM-RandomForest&#39;.\nAccuracy = 0.954281\nPrecision = 0.905537\nRecall = 0.276961\nF1 = 0.424185\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registered model &#39;PM-RandomForest&#39; already exists. Creating a new version of this model...\nCreated version &#39;2&#39; of model &#39;PM-RandomForest&#39;.\nAccuracy = 0.954281\nPrecision = 0.905537\nRecall = 0.276961\nF1 = 0.424185\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["After running the cell above go back to your Azure ML Workspace you should be able to see your runs. The runs are also logged on ML Flow UI on your Azure Databricks workspace. If you click on the `Experiment` symbol above, followed by the arrow showing `View Run Detail` you will be able to view the run details on your Datbricks Workspace.  \n\n\n![](https://github.com/felicity-borg/BatchSparkScoringPredictiveMaintenance/blob/master/images/AzureML_Experiments.PNG?raw=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61f2a809-4545-4391-bda9-cb19400d6d47"}}},{"cell_type":"markdown","source":["###Build an Azure Container Image for model deployment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"424ac99c-9d58-4df1-b966-3118f224d1bc"}}},{"cell_type":"markdown","source":["### Use MLflow to build a Container Image for the trained model\n\nUse the `mlflow.azuereml.build_image` function to build an Azure Container Image for the trained MLflow model. This function also registers the MLflow model with a specified Azure ML workspace. The resulting image can be deployed to Azure Container Instances (ACI) or Azure Kubernetes Service (AKS) for real-time serving."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bf96da7-3e46-46d6-b322-642eb7063356"}}},{"cell_type":"markdown","source":["Specify the last run ID associated with the model's training. You can find a run ID and model path from the experiment run, which can be found on the run details.\n\n![](https://docs.azuredatabricks.net/_static/images/mlflow/mlflow-deployment-example-run-info.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"743b6a63-0609-4483-91c4-01fc72cbf10f"}}},{"cell_type":"code","source":["# The latest run ID\nrun_id1 = \"b05532fdd06e41be9d9c31ae3425c631\"\n# Builds URI for the model associated with the latest run\nmodel_uri = \"runs:/\" + run_id1 + \"/model\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbccab14-cfdf-4b02-b8f0-6803084067ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use the `mlflow.azuereml.build_image` function to build an Azure Container Image for the trained MLflow model and register the MLflow model with the specified Azure ML workspace"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd2e041c-9f6a-4090-8cc6-c8a20134942f"}}},{"cell_type":"code","source":["# first part of mlflow.azureml.build_image option which is getting depracted\n\nimport mlflow.azureml\n\nazure_image, azure_model = mlflow.azureml.build_image(model_uri=model_uri, \n                                                      workspace=ws,\n                                                      image_name = \"pm-randomforest\",\n                                                      model_name= \"predictive-maintenance-model\",\n                                                      #service_name = \"predictive-maintenance\",\n                                                      description=\"RandomForest for predicting machine failure\",\n                                                      synchronous=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"590e2fdd-7746-4991-bc1c-294d2430936b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Registering model predictive-maintenance-model\n2020/11/06 12:03:50 INFO mlflow.azureml: Registered an Azure Model with name: `predictive-maintenance-model` and version: `3`\n/databricks/python/lib/python3.7/site-packages/mlflow/azureml/__init__.py:222: DeprecationWarning: ContainerImage class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  tags=tags,\n/databricks/python/lib/python3.7/site-packages/azureml/core/image/container.py:161: DeprecationWarning: ContainerImageConfig class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  base_image, base_image_registry, cuda_version=cuda_version)\n/databricks/python/lib/python3.7/site-packages/mlflow/azureml/__init__.py:228: DeprecationWarning: Image class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  models=[registered_model],\nCreating image\n/databricks/python/lib/python3.7/site-packages/azureml/core/image/image.py:407: DeprecationWarning: Image class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  image = Image(workspace, id=image_id)\n2020/11/06 12:03:53 INFO mlflow.azureml: Building an Azure Container Image with name: `pm-randomforest` and version: `3`\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Registering model predictive-maintenance-model\n2020/11/06 12:03:50 INFO mlflow.azureml: Registered an Azure Model with name: `predictive-maintenance-model` and version: `3`\n/databricks/python/lib/python3.7/site-packages/mlflow/azureml/__init__.py:222: DeprecationWarning: ContainerImage class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  tags=tags,\n/databricks/python/lib/python3.7/site-packages/azureml/core/image/container.py:161: DeprecationWarning: ContainerImageConfig class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  base_image, base_image_registry, cuda_version=cuda_version)\n/databricks/python/lib/python3.7/site-packages/mlflow/azureml/__init__.py:228: DeprecationWarning: Image class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  models=[registered_model],\nCreating image\n/databricks/python/lib/python3.7/site-packages/azureml/core/image/image.py:407: DeprecationWarning: Image class has been deprecated and will be removed in a future release. Please migrate to using Environments. https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments\n  image = Image(workspace, id=image_id)\n2020/11/06 12:03:53 INFO mlflow.azureml: Building an Azure Container Image with name: `pm-randomforest` and version: `3`\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Once successfully run you can go back to your Azure ML workspace and click on `Models` on the right hand side. <br>\nHere you can select the model you have ust registered and you will be preseneted with information about the model itself. <br>\n\n\n![](https://github.com/felicity-borg/BatchSparkScoringPredictiveMaintenance/blob/master/images/azure_ml_model.PNG?raw=true)\n\nIf you go on `Artifacts`- this is ML flow's common model format; here you will see the pickle file (if there is one) and the format of the model (the configuration of the model)\n\n![](https://github.com/felicity-borg/BatchSparkScoringPredictiveMaintenance/blob/master/images/Artificat.PNG?raw=true)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"365f1185-3b89-4083-85b5-1c27baa4a8cf"}}},{"cell_type":"markdown","source":["### Conclusion\n\nThis container image can then be deployed to Azure ML for staging and developmental model deployments using Azure Container Instances (ACI) <br>\nor using Azure Kubernetes Service (AKS) for real-time serving"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6268db2-09ec-45f9-aa65-5a461b84cf34"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3b_AzureML_Model_Building_And_Training","dashboards":[],"language":"python","widgets":{"Testing_table":{"nuid":"46446d57-f38f-404f-a9fc-895f3394dd4d","currentValue":"testing_data","widgetInfo":{"widgetType":"text","name":"Testing_table","defaultValue":"testing_data","label":null,"options":{"widgetType":"text","validationRegex":null}}},"start_date":{"nuid":"a733c065-96a1-49c2-bfdb-2dd35a5ecf7b","currentValue":"2000-01-01","widgetInfo":{"widgetType":"text","name":"start_date","defaultValue":"2015-11-30","label":null,"options":{"widgetType":"text","validationRegex":null}}},"Model":{"nuid":"4f4549c1-2555-4ed4-a6ed-ac35ca80112b","currentValue":"RandomForest","widgetInfo":{"widgetType":"text","name":"Model","defaultValue":"RandomForest","label":null,"options":{"widgetType":"text","validationRegex":null}}},"to_date":{"nuid":"4268cdd8-0dbc-426f-8f32-22bb453f4051","currentValue":"2015-10-30","widgetInfo":{"widgetType":"text","name":"to_date","defaultValue":"2016-02-01","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":4105559978712054}},"nbformat":4,"nbformat_minor":0}
